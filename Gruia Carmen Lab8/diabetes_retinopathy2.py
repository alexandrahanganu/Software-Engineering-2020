# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KA--3dhbN4YxSxs4thdfVlF_yjH3Cza0
"""

# CNN model parameters
BATCH_SIZE = 32
EPOCHS = 20  #this take over 16 hours to run, early stopping trials ended at about 6 epochs
WARMUP_EPOCHS = 2
LEARNING_RATE = 1e-4
WARMUP_LEARNING_RATE = 1e-3
HEIGHT = 128
WIDTH = 128
COLORS = 3
N_CLASSES = 5
ES_PATIENCE = 5
RLROP_PATIENCE = 3
DECAY_DROP = 0.5

# get all the images in the training directory and reshape and augment them
train_datagen = ImageDataGenerator(
      rescale=1/255,
      rotation_range=20,
#       width_shift_range=0.2,   # removed these for time savings
#       height_shift_range=0.2,
#       shear_range=0.2,
#       zoom_range=0.5,
      horizontal_flip=True,
      fill_mode='nearest')

train_generator = train_datagen.flow_from_directory(
        train_dir, 
        target_size=(HEIGHT, WIDTH), 
        batch_size= BATCH_SIZE,
        shuffle = True,
        class_mode= 'categorical') 


# get all the data in the validation directory and reshape them
val_datagen = ImageDataGenerator(rescale=1/255)

val_generator = val_datagen.flow_from_directory(
        validation_dir, 
        target_size=(HEIGHT, WIDTH),
        batch_size = BATCH_SIZE,
        class_mode= 'categorical')

# get all the data in the test directory and reshape them
test_generator = ImageDataGenerator(rescale=1/255).flow_from_directory(
        test_dir, 
        target_size=(HEIGHT, WIDTH), 
        batch_size = 1,
        class_mode= 'categorical',
        shuffle = False)
Found 2929 images belonging to 5 classes.
Found 365 images belonging to 5 classes.
Found 368 images belonging to 5 classes.

# created a function to create models - all using ResNet50 weights
def create_model(input_shape, n_out):
    input_tensor = Input(shape=input_shape)
    base_model = applications.ResNet50(weights=None, 
                                       include_top=False,
                                       input_tensor=input_tensor)
    base_model.load_weights('resnet50/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5')

    x = GlobalAveragePooling2D()(base_model.output)
    x = Dropout(0.5)(x)
    x = Dense(2048, activation='relu')(x)
    x = Dropout(0.5)(x)
    final_output = Dense(n_out, activation='softmax', name='final_output')(x)
    model = Model(input_tensor, final_output)
    
    return model

# instantiate and compile a model
model = create_model(input_shape=(HEIGHT, WIDTH, COLORS), n_out=N_CLASSES)

for layer in model.layers:
    layer.trainable = False

for i in range(-5, 0):
    model.layers[i].trainable = True

metric_list = ["accuracy"]
optimizer = optimizers.Adam(lr=WARMUP_LEARNING_RATE)
model.compile(optimizer=optimizer, loss="categorical_crossentropy",  metrics=metric_list)
model.summary()

# create the model, use early stopping
for layer in model.layers:
    layer.trainable = True

es = EarlyStopping(monitor='val_loss', mode='min', patience=ES_PATIENCE, verbose=1)
rlrop = ReduceLROnPlateau(monitor='val_loss', mode='min', patience=RLROP_PATIENCE, factor=DECAY_DROP, min_lr=1e-6, verbose=1)

callback_list = [es, rlrop]
optimizer = optimizers.Adam(lr=LEARNING_RATE)
model.compile(optimizer=optimizer, loss="binary_crossentropy",  metrics=metric_list)
model.summary()

# track the time
start = datetime.datetime.now()
print('Started predicting at {}'.format(start))

prediction = model.predict([x_test])

end = datetime.datetime.now()
elapsed = end - start
print('Predicting took a total of {}'.format(elapsed))

predictions = [np.argmax(pred) for pred in prediction]

# output the predictions for each image
results = pd.DataFrame({'id_code': test_data['id_code'], 'diagnosis':predictions})
results.to_csv('submission_v5.csv',index=False)
results.head()